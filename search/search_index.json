{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KubeFlex Platform","text":"<p>Head over to our technical guides tab for detailed information on configuring a cloud-agnostic setup. </p>"},{"location":"about/","title":"About","text":"<p>Kubeflex is a community-driven platform focused on empowering startups. We take great pleasure in seeing startups flourish.</p>"},{"location":"certifications/","title":"Certification Offers","text":"Certification Discounts Directory <p>Explore exclusive offers to support your growth.</p> <p>Offers marked with a heart icon \u2764\ufe0f are affiliate links, meaning we earn a commission if you make a purchase through them. Thank you for your support!</p> <p> All <p></p>"},{"location":"pricing/","title":"Pricing","text":"Absolutely FreePaid Customer - PlatformPaid Customer - Platform + Development <pre><code>* We place you in a queue\n* We build the entire platform\n* We share the documentation\n</code></pre> <pre><code>* No queue. Immediate involvement\n* We build the entire platform\n* We share the documentation\n* We accommodate custom needs\n* We maintain the platform\n</code></pre> <pre><code>* No queue. Immediate involvement\n* We build the entire platform\n* We share the documentation\n* We accommodate custom needs\n* We maintain the platform\n* We develop your applications\n</code></pre> <p>Show your interest!</p>"},{"location":"startup-programs/","title":"Startup Programs","text":"Startup Programs Directory <p>Explore exclusive offers to support your startup's growth.</p> <p>Offers marked with a heart icon \u2764\ufe0f are affiliate links, meaning we earn a commission if you make a purchase through them. Thank you for your support!</p> <p> All Analytics Bug Tracking Business Suites Cloud Computing Cloud Database Cloud Communication Customer Engagement Incident Management Legal and Compliance Marketing and Sales Payments Productivity Software Video 3D Modeling <p></p>"},{"location":"trainings/","title":"Training Offers","text":"Training Discounts Directory <p>Explore exclusive offers to support your growth.</p> <p>Offers marked with a heart icon \u2764\ufe0f are affiliate links, meaning we earn a commission if you make a purchase through them. Thank you for your support!</p> <p> All <p></p>"},{"location":"technical-guides/","title":"Welcome KubeFlex Technical Documentation","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"technical-guides/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"technical-guides/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"technical-guides/auth/","title":"Authentication and Authorization with Istio and Keycloak","text":"<p>In this guide, we will explore how to leverage Istio to implement authentication and authorization using Keycloak. The goal is to simplify development, allowing developers to focus on their core tasks without worrying about authentication and authorization. We will cover this step-by-step with practical examples and working sample codes.</p> <p></p>"},{"location":"technical-guides/auth/#contents","title":"Contents","text":"<ul> <li> Introduction to Keycloak</li> <li> Introduction to Istio</li> <li> Introduction to FastAPI</li> <li> Deploying the job-service Microservice without Authentication and Authorization<ul> <li> Istio Weight-Based Traffic Routing between job-service-v1 and job-service-v2</li> </ul> </li> <li> Implementing Authentication with Istio<ul> <li> Passing the JWT Token to Backend Services</li> </ul> </li> <li> Improving the Code with Additional Constraints<ul> <li> Decoding the Token to get the Logged-in User</li> <li> Validate the Ownership of an Item</li> </ul> </li> <li> Implementing Authorization with Istio (Based on Keycloak Roles)</li> <li> Implementing Authorization between Microservices</li> </ul>"},{"location":"technical-guides/auth/#introduction-to-keycloak","title":"Introduction to Keycloak","text":"<p>Keycloak is an open-source identity and access management solution that offers single sign-on (SSO) capabilities, allowing users to authenticate once and access multiple applications and services with a single set of credentials. One of the features I find particularly impressive is Keycloak's ability to simplify the development process by enabling the integration of custom themes for the authentication flow, such as the login page. In this scenario, we have deployed Keycloak within the same Kubernetes cluster.</p> <p>Following is the Dockerfile which we use to build a custom Keycloak image with our own theme and the event listener.  <pre><code>FROM quay.io/keycloak/keycloak:24.0.3\nCOPY ./kubeflex-theme /opt/keycloak/themes/kubeflex-theme\nCOPY ./providers/create-account-custom-spi.jar /opt/keycloak/providers/create-account-custom-spi.jar\n</code></pre> We use the following deployment manifest to deploy Keycloak. </p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keycloak\n  namespace: keycloak\n  labels:\n    app: keycloak\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keycloak\n  template:\n    metadata:\n      labels:\n        app: keycloak\n    spec:\n      serviceAccountName: keycloak\n      automountServiceAccountToken: true\n      containers:\n        - name: keycloak\n          image: eocontainerregistry.azurecr.io/keycloak:v1.8.6 # {\"$imagepolicy\": \"flux-system:keycloak\"}\n          args: [\"start\"]\n          env:\n            - name: KEYCLOAK_ADMIN\n              value: \"admin\"\n            - name: KEYCLOAK_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: keycloak\n                  key: admin-password\n            - name: KC_HOSTNAME\n              value: auth.kubeflex.io\n            - name: KC_PROXY\n              value: \"edge\"\n            - name: KC_DB\n              value: mysql\n            - name: KC_DB_URL\n              value: \"jdbc:mysql://keycloakdb-keycloakdb-mysql.keycloakdb.svc.cluster.local:3306/keycloakdb\"\n            - name: KC_DB_USERNAME\n              value: \"keycloak-user\"\n            - name: jgroups.dns.query\n              value: keycloak\n            - name: KC_DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: keycloak\n                  key: db-password\n          ports:\n            - name: http\n              containerPort: 8080\n            - name: jgroups\n              containerPort: 7600\n      imagePullSecrets:\n        - name: acr-secret\n</code></pre> Info <p>Notice FluxCD <code>imagepolicy</code> reference in the manifest file. With this, we can automate the deployment whenever a new image is available in the image repository. </p>"},{"location":"technical-guides/auth/#introduction-to-istio","title":"Introduction to Istio","text":"<p>Istio is an open-source service mesh platform designed to manage how microservices communicate and share data. It provides a variety of features to improve the observability, security, and management of microservice applications. We will soon discuss how we have configured Istio.</p>"},{"location":"technical-guides/auth/#introduction-to-fastapi","title":"Introduction to FastAPI","text":"<p>FastAPI is a modern Python framework that is rapidly gaining popularity. It is designed for rapid development and to maximize the developer experience. In this example, we will use two versions of a job API (V1,  V2,) written in FastAPI. The API utilizes the SQLModel library to interact with the backend database, combining features from both SQLAlchemy and Pydantic. </p> Info <p>SQLModel is developed by the same author as FastAPI. </p>"},{"location":"technical-guides/auth/#deploying-job-service-without-authentication-and-authorization","title":"Deploying job-service without Authentication and Authorization","text":"<p>Let's start with something simpler: a working microservice without any authentication or authorization. </p> <p>Here are the deployment manifests for job-service v1 and job-service v2, both running in the \"job-service\" namespace. Take note of the version label in each deployment manifest.  <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: job-service\n    version: v1\n  name: job-service\n  namespace: job-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: job-service\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: job-service\n        version: v1\n    spec:\n      serviceAccountName: job-service\n      automountServiceAccountToken: true\n      containers:\n      - image: eocontainerregistry.azurecr.io/job-service:v1.0.3 # {\"$imagepolicy\": \"flux-system:job-service-v1\"}\n        name: job-service\n        env:\n        - name: DB_HOST\n          value: \"keycloakdb-keycloakdb-mysql.keycloakdb.svc.cluster.local\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: job-service\n              key: db-password\n        - name: DB_PORT\n          value: \"3306\"\n        - name: DB_USER\n          value: \"job-service\"\n        - name: DB_NAME\n          value: \"job-service\"\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"50m\"\n      imagePullSecrets:\n      - name: acr-secret\n</code></pre></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: job-service\n    version: v2\n  name: job-service-v2\n  namespace: job-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: job-service\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: job-service\n        version: v2\n    spec:\n      serviceAccountName: job-service\n      automountServiceAccountToken: true\n      containers:\n      - image: eocontainerregistry.azurecr.io/job-service-v2:v1.1.3 # {\"$imagepolicy\": \"flux-system:job-service-v2\"}\n        name: job-service\n        env:\n        - name: DB_HOST\n          value: \"keycloakdb-keycloakdb-mysql.keycloakdb.svc.cluster.local\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: job-service\n              key: db-password\n        - name: DB_PORT\n          value: \"3306\"\n        - name: DB_USER\n          value: \"job-service\"\n        - name: DB_NAME\n          value: \"job-service\"\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"50m\"\n      imagePullSecrets:\n      - name: acr-secret\n</code></pre> <p>We also have a ClusterIP service with the selector \"app=job-service.\" This configuration ensures that both job-service v1 and job-service v2 are added as endpoints of this service.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: job-service\n    kustomize.toolkit.fluxcd.io/name: flux-system\n    kustomize.toolkit.fluxcd.io/namespace: flux-system\n  name: job-service\n  namespace: job-service\nspec:\n  ports:\n  - port: 80\n    name: http\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: job-service\n  type: ClusterIP\n</code></pre> <p>Additionally, note that we are using the same database instance for both Keycloak and the job-service versions. However, the respective users are restricted from accessing each other's databases. This setup, despite sharing the same instance, effectively mimics a microservice architecture.</p> <p>For the first step, we want to route all traffic exclusively to the v1 deployment. (If you look at job-service v1, you'll see that it has been written without any authentication or authorization in the code. We plan to implement these features using Istio in the upcoming steps.)</p> <p>To achieve this, we create a virtual service and a destination rule as follows. </p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: job-service\n  namespace: job-service\nspec:\n  hosts:\n    - \"www.kubeflex.io\"\n    - \"kubeflex.io\"\n    - job-service.job-service.svc.cluster.local\n  gateways:\n    - istio-system/gateway\n    - mesh\n  http:\n    - match:\n        - uri:\n            prefix: \"/api/jobs\"\n        - uri:\n            prefix: \"/api/jobcategories\"\n      route:\n        - destination:\n            host: job-service.job-service.svc.cluster.local\n            subset: v1\n          weight: 100\n        - destination:\n            host: job-service.job-service.svc.cluster.local\n            subset: v2\n          weight: 0\n</code></pre> <p><pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: job-service\n  namespace: job-service\nspec:\n  host: job-service.job-service.svc.cluster.local\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n</code></pre> Note that under the gateways section, we specify both our ingress gateway and \"mesh.\" This is because we expect traffic from both the external gateway and other microservices within the cluster. Observe how we have directed 100% of the traffic to the v1 deployment.</p> <p>Below is Istio gateway resource. It handles traffic destined for the kubeflex.io domain. Additionally, we've attached a Let's Encrypt TLS certificate to the gateway using cert-manager. <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n    - port:\n        number: 443\n        name: https\n        protocol: HTTPS\n      tls:\n        mode: SIMPLE\n        credentialName: kubeflex-tls\n      hosts:\n      - \"www.kubeflex.io\"\n      - \"kubeflex.io\"\n      - \"auth.kubeflex.io\"\n</code></pre> We can use Kiali dashboard to validate our routing configuration. Note that, <code>job-service</code> connects to the same <code>keycloakdb</code> MySQL instance. But in reality, <code>job-service</code> has access only to it's specific <code>job-service</code> database inside the instance. </p> <p></p> <p>Now we are ready to do some testing. </p>"},{"location":"technical-guides/auth/#creating-a-new-job-category","title":"Creating a new job category","text":"<pre><code> curl --location 'https://kubeflex.io/api/jobcategories/' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"name\": \"Information Technology\"\n}'\n{\"name\":\"Information Technology\",\"id\":17}\n</code></pre>"},{"location":"technical-guides/auth/#creating-a-new-job","title":"Creating a new job","text":"<pre><code>curl --location 'https://kubeflex.io/api/jobs/' \\ 'https://kubeflex.io/api/jobs/' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"title\": \"Software Engineer\",\n  \"description\": \"Software Engineer with 2 years of experience\",\n  \"owner_id\": \"5690cc29-5008-4a81-8f08-db92e01d6d44\",\n  \"category_id\": 17\n}'\n{\"title\":\"Software Engineer\",\"description\":\"Software Engineer with 2 years of experience\",\"owner_id\":\"5690cc29-5008-4a81-8f08-db92e01d6d44\",\"category_id\":17,\"id\":\"f19e68da-e40a-4954-9dbf-6dfaf1f7f4d4\"}\n</code></pre>"},{"location":"technical-guides/auth/#get-job-category-by-id","title":"Get job category by ID","text":"<pre><code>curl --location 'https://kubeflex.io/api/jobcategories/17'\n{\"name\":\"Information Technology\",\"id\":17}\n</code></pre>"},{"location":"technical-guides/auth/#get-job-by-id","title":"Get job by ID","text":"<pre><code>curl --location 'https://kubeflex.io/api/jobs/bff285f6-34f6-4c5f-9619-2e860bec2d87'\n{\"title\":\"Software Engineer\",\"description\":\"Software Engineer with 2 years of experience\",\"owner_id\":\"5690cc29-5008-4a81-8f08-db92e01d6d44\",\"category_id\":17,\"id\":\"bff285f6-34f6-4c5f-9619-2e860bec2d87\"}\n</code></pre> <p>As you can see, there is no authentication or authorization on these endpoints. Anyone can create, update, delete, or retrieve jobs and job categories.</p> <p>Info</p> <p>Note: I used Swagger, which is integrated with FastAPI, to generate the sample curl requests.</p> <p>Also, please note that when creating new jobs, we manually pass the <code>owner_id</code> with the request. Ideally, this should be the user ID of the logged-in user. We will delve further into this when discussing job-service v2.</p>"},{"location":"technical-guides/auth/#implementing-authentication-with-istio","title":"Implementing Authentication with Istio","text":"<p>Let's secure our endpoints.</p> <p>Firstly, let's add the RequestAuthentication resource, which defines the supported request authentications for the workload. This configuration ensures that Istio rejects any request with invalid authentication information. Below, we have defined our Keycloak issuer URL and public certificate URL to enable Istio to verify the token signature.</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: RequestAuthentication\nmetadata:\n  name: job-service\n  namespace: job-service\nspec:\n  selector:\n     matchLabels:\n      app: job-service\n  jwtRules:\n   - issuer: \"https://auth.kubeflex.io/realms/kubeflex\"\n     jwksUri: \"https://auth.kubeflex.io/realms/kubeflex/protocol/openid-connect/certs\"\n     forwardOriginalToken: true\n</code></pre> <p>Additionally, we have set \"forwardOriginalToken\": true, as we need to pass the token in the format \"Authorization: Bearer \" to the backend service. You can also pass the token to the backend service under a custom header name. For instance, you can use the following code snippet to pass the token as a value of the key \"jwt_parsed\": <pre><code>   jwtRules:\n    - issuer: \"https://auth.kubeflex.io/realms/kubeflex\"\n      jwksUri: \"https://auth.kubeflex.io/realms/kubeflex/protocol/openid-connect/certs\"\n      outputPayloadToHeader: jwt-parsed\n</code></pre> <p>Now, RequestAuthentication will reject any request with an invalid token. However, requests without any authentication information will still be accepted, but they won't have an authenticated identity. To handle these cases, in addition to RequestAuthentication, we need to drop requests that lack an authentication identity. Therefore, we add an authorization policy as follows:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: job-service\n  namespace: job-service\nspec:\n  selector:\n    matchLabels:\n       app: job-service\n  rules:\n  - to:\n    - operation:\n        methods: [\"GET\"]\n  - from:\n    - source:\n        requestPrincipals: [\"*\"]\n    to:\n    - operation:\n        methods: [\"POST\", \"DELETE\", \"PATCH\"]\n        paths: [\"/api/jobs*\"]\n    - operation:\n        methods: [\"POST\", \"DELETE\", \"PATCH\"]\n        paths: [\"/api/jobcategories*\"]\n</code></pre> <p>So, with the above AuthorizationPolicy, we have allowed unrestricted access to the GET method for anyone. However, authentication is required for any other methods on the job and jobcategory endpoints.</p> <p>Let's test some endpoints:</p>"},{"location":"technical-guides/auth/#creating-a-new-job_1","title":"Creating a new job","text":"<pre><code>curl --location 'https://kubeflex.io/api/jobs/' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"title\": \"Software Engineer II\",\n  \"description\": \"Software Engineer with 2 years of experience\",\n  \"owner_id\": \"5690cc29-5008-4a81-8f08-db92e01d6d44\",\n  \"category_id\": 17\n}'\nRBAC: access denied\n</code></pre>"},{"location":"technical-guides/auth/#get-job-by-id_1","title":"Get job by ID","text":"<pre><code>curl --location 'https://kubeflex.io/api/jobs/bff285f6-34f6-4c5f-9619-2e860bec2d87'\n{\"title\":\"Software Engineer\",\"description\":\"Software Engineer with 2 years of experience\",\"owner_id\":\"5690cc29-5008-4a81-8f08-db92e01d6d44\",\"category_id\":17,\"id\":\"bff285f6-34f6-4c5f-9619-2e860bec2d87\"}\n</code></pre>"},{"location":"technical-guides/auth/#creating-a-new-job-category_1","title":"Creating a new job category","text":"<pre><code> curl --location 'https://kubeflex.io/api/jobcategories/' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"name\": \"Computer Science\"\n}'\nRBAC: access denied\n</code></pre>"},{"location":"technical-guides/auth/#get-job-category-by-id_1","title":"Get job category by ID","text":"<pre><code>curl --location 'https://kubeflex.io/api/jobcategories/17'\n{\"name\":\"Information Technology\",\"id\":17}\n</code></pre> <p>As observed, we can retrieve information without authentication. However, authentication is necessary for adding, modifying, or deleting entries.</p> <p>Next, let's generate a token by calling the Keycloak token URL and use it to perform add, modify, or delete operations:</p>"},{"location":"technical-guides/auth/#generating-a-token","title":"Generating a token","text":"<p><pre><code>curl --location 'https://auth.kubeflex.io/realms/kubeflex/protocol/openid-connect/token' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data-urlencode 'grant_type=password' \\\n--data-urlencode 'client_id=kubeflex-platform' \\\n--data-urlencode 'username=&lt;username&gt;' \\\n--data-urlencode 'password=&lt;password&gt;'\n</code></pre> This returns an access token, which we can use for subsequent requests. </p>"},{"location":"technical-guides/auth/#creating-a-new-job_2","title":"Creating a new job","text":"<p><pre><code>curl --location 'https://kubeflex.io/api/jobs/' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;token&gt;' \\\n--data '{\n  \"title\": \"Software Engineer II\",\n  \"description\": \"Software Engineer with 2 years of experience\",\n  \"owner_id\": \"5690cc29-5008-4a81-8f08-db92e01d6d44\",\n  \"category_id\": 17\n}'\n\n{\n    \"title\": \"Software Engineer II\",\n    \"description\": \"Software Engineer with 2 years of experience\",\n    \"owner_id\": \"5690cc29-5008-4a81-8f08-db92e01d6d44\",\n    \"category_id\": 17,\n    \"id\": \"571c9ce6-566f-4e57-a780-9af5275ce5ef\"\n}\n</code></pre> As demonstrated, authenticated users are able to successfully add, modify, or delete jobs and job categories.</p>"},{"location":"technical-guides/auth/#improving-the-code-with-additional-constraints","title":"Improving the Code with Additional Constraints","text":"<p>At this point, if you examine the source code of (job-service-v1, you'll notice it focuses solely on core functionality without incorporating authentication and authorization concerns, which are managed entirely by Istio. However, this approach has some drawbacks. </p> <p>We currently need to manually provide the owner_id when creating a job, whereas ideally, this should be automatically set to the ID of the logged-in user. Moreover, a user should not have the ability to modify a job created by someone else. This necessitates that the backend service is aware of the logged-in user's ID and can enforce this constraint. We have addressed these issues in the v2 service. Please see the implementation here.</p>"},{"location":"technical-guides/auth/#populate-owner_id-during-job-creation","title":"Populate owner_id during job creation","text":"<p><pre><code>@router.post(\"/\", response_model=JobPublic)\ndef create_job(*, session: Session = Depends(get_session), job: JobCreate, request: Request, user_id: str = Depends(get_user_id_from_token)):\n    db_job = Job.model_validate(job)\n    db_job.id = str(uuid.uuid4())\n    db_job.owner_id = user_id\n    session.add(db_job)\n    session.commit()\n    session.refresh(db_job)\n\n    return db_job\n</code></pre> In this post method, we have included <code>get_user_id_from_token</code> as a dependency to inject the <code>user_id</code> into <code>owner_id</code> during job creation.</p> <pre><code>def get_user_id_from_token(request: Request) -&gt; str:\n    try:\n        token = request.headers.get(\"Authorization\").split(\"Bearer \")[1]\n        payload = jwt.decode(token, options={\"verify_signature\": False})\n        user_id: str = payload.get(\"sub\")\n        if user_id is None:\n            raise HTTPException(status_code=401, detail=\"User ID not found in token\")\n        return user_id\n    except:\n        raise HTTPException(status_code=401, detail=\"Could not validate credentials\")\n</code></pre>"},{"location":"technical-guides/auth/#validate-the-ownership","title":"Validate the ownership","text":"<p>In the following PATCH method, we validate whether the job is owned by the logged-in user. If not, the user is not allowed to modify the job.</p> <pre><code>@router.patch(\"/{job_id}\", response_model=JobPublic)\ndef update_job(*, session: Session = Depends(get_session), job_id: str, job: JobUpdate, request: Request, user_id: str = Depends(get_user_id_from_token)):\n    db_job = session.get(Job, job_id)\n    if not db_job:\n        raise HTTPException(status_code=404, detail=\"Job not found\")\n    if db_job.owner_id != user_id:\n        raise HTTPException(status_code=403, detail=\"You do not have permission to update this job\")\n    job_data = job.model_dump(exclude_unset=True)\n    db_job.sqlmodel_update(job_data)\n    session.add(db_job)\n    session.commit()\n    session.refresh(db_job)\n    return db_job\n</code></pre> <p>Let's route all traffic to job-service v2. We can achieve this by modifying the virtual service. <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: job-service\n  namespace: job-service\nspec:\n  hosts:\n    - \"www.kubeflex.io\"\n    - \"kubeflex.io\"\n    - job-service.job-service.svc.cluster.local\n  gateways:\n    - istio-system/gateway\n    - mesh\n  http:\n    - match:\n        - uri:\n            prefix: \"/api/jobs\"\n        - uri:\n            prefix: \"/api/jobcategories\"\n      route:\n        - destination:\n            host: job-service.job-service.svc.cluster.local\n            subset: v1\n          weight: 0\n        - destination:\n            host: job-service.job-service.svc.cluster.local\n            subset: v2\n          weight: 100\n</code></pre> Now, Let's try to create a job without <code>owner_id</code>. Please note that, now <code>JobCreate</code> data model no longer has <code>owner_id</code> attribute. </p> <p><pre><code> curl --location 'https://kubeflex.io/api/jobs/' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;token&gt;' \\\n--data '{\n  \"title\": \"Software Engineer III\",\n  \"description\": \"Software Engineer with 2 years of experience\",\n  \"category_id\": 17\n}'\n{\"title\":\"Software Engineer III\",\"description\":\"Software Engineer with 2 years of experience\",\"category_id\":17,\"id\":\"b3b1baa8-91bc-42bd-9736-94f13b39b610\",\"owner_id\":\"78de9a7a-7bcb-4b61-9c27-478704a1986a\"}\n</code></pre> As you can see, we do not require to specify the <code>owner_id</code>. FastAPI automatically inject the ID of the logged-in user. </p> <p>Let's try to modify a job </p> <p>Let's attempt to modify a job owned by someone else.</p> <p><pre><code>curl --location --request PATCH 'https://kubeflex.io/api/jobs/2f736c4d-11ba-421f-953a-d1d6f0e5b653' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;token&gt;' \\\n--data '{\n  \"title\": \"Software Engineer IV\"\n}'\n{\"detail\":\"You do not have permission to update this job\"}\n</code></pre> As you can see, we are unable to modify jobs owned by someone else.</p>"},{"location":"technical-guides/auth/#implementing-authorization-with-istio","title":"Implementing Authorization with Istio","text":"<p>At this point, we have configured authentication. When it comes to job categories, we do not expect a large number to be present in the database. It makes sense to maintain a limited number of job categories and restrict creation, modification, and deletion to admin users only.</p> <p>Currently, anyone with valid authentication can modify job categories. Let's see how we can implement authorization.</p> <p>We modify our authorization policy to ensure that only users with the admin role can modify job categories. Alternatively, you can use \"groups\" if you have a more complex user hierarchy.</p> <p><pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: job-service\n  namespace: job-service\nspec:\n  selector:\n    matchLabels:\n      app: job-service\n  rules:\n  - to:\n    - operation:\n        methods: [\"GET\"]\n  - from:\n    - source:\n        requestPrincipals: [\"*\"]\n    to:\n    - operation:\n        methods: [\"POST\", \"DELETE\", \"PATCH\"]\n        paths: [\"/api/jobs*\"]\n  - from:\n    - source:\n        requestPrincipals: [\"*\"]\n    when:\n    - key: request.auth.claims[realm_access][roles]\n      values: [\"admin\"]\n    to:\n    - operation:\n        methods: [\"POST\", \"DELETE\", \"PATCH\"]\n        paths: [\"/api/jobcategories*\"]\n</code></pre> We can create the admin role by navigating to \"Realm Roles\" under the Keycloak realm we use. After that, we go to the respective user we want to assign the admin role to, click on the \"Role Mapping\" tab, and add the user to the newly created \"admin\" role.</p> <p></p> <p></p>"},{"location":"technical-guides/auth/#creating-a-job-category-regular-user","title":"Creating a Job Category - Regular User","text":"<pre><code>curl --location 'https://kubeflex.io/api/jobcategories/' \\location 'https://kubeflex.io/api/jobcategories/' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;token&gt;' \\\n--data '{\n    \"name\": \"Research\"\n}'\nRBAC: access denied\n</code></pre>"},{"location":"technical-guides/auth/#creating-a-job-category-admin-user","title":"Creating a Job Category - Admin User","text":"<p><pre><code>curl --location 'https://kubeflex.io/api/jobcategories/' \\i/jobcategories/' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;token&gt;' \\\n--data '{\n    \"name\": \"Research\"\n}'\n{\"name\":\"Research\",\"id\":20}\n</code></pre> As we can see, only admin users can create/update/delete job categories now. </p>"},{"location":"technical-guides/auth/#implementing-authorization-between-microservices","title":"Implementing Authorization between MicroServices","text":"<p>Now we add another microservice called \"job-notification-service\".  </p> <p>This service is not exposed via the gateway and should be accessible only by \"job-service\". To achieve this, we can add the following authorization policy.</p> <p><pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: job-notification-service\n  namespace: job-notification-service\nspec:\n  selector:\n    matchLabels:\n      app: job-notification-service\n  rules:\n  - from:\n    - source:\n        namespaces: [\"job-service\"]\n        principals: [\"cluster.local/ns/job-service/sa/job-service\"]\n</code></pre> PeerAuthentication to enforce mTLS between two services <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: job-notification-service-mtls\n  namespace: job-notification-service\nspec:\n  selector:\n    matchLabels:\n      app: job-notification-service\n  mtls:\n    mode: STRICT\n</code></pre></p> Note <p>Please note that if the job-notification-service requires the requester's details, we must pass the token from the job-service to the job-notification-service programmatically. Istio, by default, will only propagate the JWT token for one hop.</p> Tip <p>Alternatively, you can decode the token in the job-service and pass the decoded user details when calling the job-notification-service APIs.</p>"},{"location":"technical-guides/auth/#conclusion","title":"Conclusion","text":"<p>In conclusion, we have implemented authentication and authorization for our microservices using Istio and Keycloak, ensuring secure access to resources. We've configured policies to control access based on roles and user identities, enhancing the overall security posture of our applications.</p> <p>I welcome any feedback you may have regarding areas for improvement, any aspects that may have been overlooked, or suggestions to enhance this document. </p> <p>Note: This page is part of Cloud Agnostic Platform guide. Click here to access the main page.</p>"},{"location":"technical-guides/cert-manager/","title":"Cert Manager","text":"<p>Cert manager is a Kubernetes native certificate management controller. It can automatically provision, manage and renew TLS certificates from various certificate authorities (CA). We have configured cert-manager to obtain Let's encrypt certificates for our public endpoints. </p>"},{"location":"technical-guides/cert-manager/#helm-repository","title":"Helm Repository","text":"<p>Directory structure <pre><code>clusters\n--production\n----flux-system\n------helm-repositories\n--------cert-manager.yaml\n------kustomization.yaml\n</code></pre></p> <p>Content of cert-manager.yaml</p> <p><pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: cert-manager\n  namespace: flux-system\nspec:\n  interval: 1h0m0s\n  url: https://charts.jetstack.io\n</code></pre> Content of kustomization.yaml <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - gotk-components.yaml\n  - gotk-sync.yaml\n  - helmrepositories/cert-manager.yaml\n</code></pre></p>"},{"location":"technical-guides/cert-manager/#namespace","title":"Namespace","text":"<p>Let's create a namespace for cert-manager deployment</p> <p>Directory structure <pre><code>clusters\n--production\n----cert-manager\n------namespace.yaml\n</code></pre> Content of namespace.yaml <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: cert-manager\n</code></pre> Note: We do not want to inject istio sidecar to this namespace. </p>"},{"location":"technical-guides/cert-manager/#helmrelease","title":"Helmrelease","text":"<p>Let's deploy cert-manager with helmrelease CRD</p> <p>Directory structure <pre><code>clusters\n--production\n----cert-manager\n------namespace.yaml\n------cert-manager.yaml\n</code></pre></p> <p>Content of cert-manager.yaml <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: cert-manager\n  namespace: flux-system\nspec:\n  chart:\n    spec:\n      chart: cert-manager\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: cert-manager\n        namespace: flux-system\n      version: 1.14.4\n  interval: 1m0s\n  releaseName: cert-manager\n  targetNamespace: cert-manager\n  values:\n    installCRDs: true\n</code></pre></p> <p>Once the above changes are merged, FluxCD will deploy cert-manager on cert-manager namespace. </p>"},{"location":"technical-guides/cert-manager/#cluster-issuers","title":"Cluster Issuers","text":"<p>We need to configure which CA (Certificate Authorities) cert-manager can work with. Let's create two cluster issuer manifests for both Let's encrypt production CA and Staging CA</p> <p>Directory structure <pre><code>clusters\n--production\n----istio-system\n------prod-cluster-issuer.yaml\n------staging-cluster-issuer.yaml\n</code></pre></p> <p>These are cluster wide resources. </p> <p>Content of prod-cluster-issuer.yaml <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod-cluster\n  namespace: istio-system\nspec:\n  acme:\n    email: ranatunga@kubeflex.io\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod-cluster\n    solvers:\n    - http01:\n        ingress:\n          class: istio\n</code></pre></p> <p>Content of staging-cluster-issuer.yaml <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging-cluster\n  namespace: istio-system\nspec:\n  acme:\n    email: ranatunga@kubeflex.io\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-staging-cluster\n    solvers:\n    - http01:\n        ingress:\n          class: istio\n</code></pre> With this, we are ready to obtain certificates for our public endpoints. We will discuss this in a later section. </p>"},{"location":"technical-guides/circleci/","title":"CircleCI","text":"<p>Let's take a look at our deployment cicle.  1. We do code changes on a feature branch and that branch is merged to the main branch. 2. CircleCI kicks in and creates a container image. 3. CircleCI pushes the image into Azure container registry 4. We note down the image tag and update the corresponding kubernetes deployment manifest in the paas-config repository. 5. FluxCD kicks in and update the corresponding kubernetes deployment.</p> <p>Note: We are planning to use Image Reflector Controller and Image Automation Controller to automate this further. </p> <p>Let's take a look at how we have configured CircleCI. This is a minimalistic configuration. We are planning to improve the setup with Orbs later on. </p>"},{"location":"technical-guides/circleci/#service-repository-configuration","title":"Service Repository Configuration","text":"<p>We have the following config.yml in each of our repositories. </p> <p>Directory structure <pre><code>.circleci\n--config.yml\n</code></pre></p> <p>Content of .config.yml <pre><code>version: 2.1\n\njobs:\n  build-and-push:\n    docker:\n      - image: cimg/node:21.7.1\n    steps:\n      - setup_remote_docker\n      - checkout\n      - run:\n          name: Login to Azure Container Registry\n          command: docker login -u $AZURE_ACR_USERNAME -p $AZURE_ACR_PASSWORD $AZURE_ACR_LOGIN_SERVER\n      - run:\n          name: Build Docker image\n          command: |\n            docker build -t $CIRCLE_PROJECT_REPONAME .\n      - run:\n          name: Tag Docker image\n          command: |\n            docker tag $CIRCLE_PROJECT_REPONAME $AZURE_ACR_LOGIN_SERVER/$CIRCLE_PROJECT_REPONAME:$CIRCLE_SHA1\n      - run:\n          name: Push Docker image to Azure Container Registry\n          command: |\n            docker push $AZURE_ACR_LOGIN_SERVER/$CIRCLE_PROJECT_REPONAME:$CIRCLE_SHA1\n\nworkflows:\n  build-and-push-image:\n    jobs:\n      - build-and-push:\n          context:\n            - registry\n          filters:\n            branches:\n              only:\n                - main\n</code></pre> Above environment variables comes from CircleCI context called \"registry\". </p>"},{"location":"technical-guides/circleci/#circleci-configuration","title":"CircleCI Configuration","text":"<p> Where</p> <ol> <li>AZURE_ACR_USERNAME is the username of Azure Container Registry</li> <li>AZURE_ACR_PASSWORD is the password of Azure Container Registry</li> <li>AZURE_ACR_LOGIN_SERVER is the Azure Container Registry</li> <li>CIRCLE_PROJECT_REPONAME is a built-in environment variable. This is the name of the repository of the current project.</li> <li>CIRCLE_SHA1 is a built-in environment variable. This is the SHA1 hash of the last commit of the current build</li> </ol>"},{"location":"technical-guides/circleci/#notes","title":"Notes","text":"<p>Please note that we changed our release process later to use semantic versioning. See the docs</p>"},{"location":"technical-guides/fluxcd/","title":"FluxCD Installation","text":"<p>FluxCD is a continuous delivery tool that automates the deployment and lifecycle management of applications on Kubernetes. It uses GitOps principles to synchronize application code stored in Git repositories with Kubernetes clusters, ensuring consistency and reliability in the deployment process. </p>"},{"location":"technical-guides/fluxcd/#installation-steps","title":"Installation Steps","text":"<ol> <li>Install Flux CLI</li> </ol> <p><pre><code>curl -s https://fluxcd.io/install.sh | sudo bash\n</code></pre> 2. Create a GitHub Personal Access Token Flux bootstrap command needs a PAT to access GitHub API.  Click on profile -&gt; Settings -&gt; Developer Settings and create a Classic Personal Access Token. And then export the token as an environment variable <pre><code>export GITHUB_TOKEN=&lt;gh-token&gt;\n</code></pre> 3. Bootstrap Flux</p> <p><pre><code>flux bootstrap github \\\n  --token-auth \\\n  --owner=kubeflex \\\n  --repository=paas-config \\\n  --branch=main \\\n  --path=clusters/production \\\n  --personal\n</code></pre> With this, paas-config GH repository in kubeflex organization will be initialized. </p> <p>After this is done, you would be able to see the following files in the new repository</p> <pre><code>clusters\n-- production\n----flux-system\n------gotk-components.yaml\n------gotk-sync.yaml\n------kustomization.yaml\n</code></pre> <p>Now we can start deploying resources into the Kubernetes cluster by pushing the changes to this github repository. Also please see the docs to learn how flux contributes to our automated release process. </p>"},{"location":"technical-guides/fluxcd/#configuring-notifications","title":"Configuring notifications","text":"<p>It is helpful to receive notifications on the status of our GitOps pipelines. For this we make use of Flux Notification Controller to send notifications to our slack. </p> <ol> <li>Create a slack channel. E.g. #flux-notifications</li> <li>Create a new slack application by visiting this. Give it an appropriate name. E.g.: FluxCD</li> <li>Navigate to Oauth &amp; Permissions section in the slack app and provide channels:read, chat:write, chat:write.customize permissions.</li> <li>Install the slack app to the slack workspace and note down Bot User Oauth Token.</li> <li>Install the application to #flux-notifications channel.</li> <li>Create a secret in flux-system namespace with the above Bot User Oauth Token</li> </ol> <p>Directory Structure <pre><code>clusters\n--production\n----flux-system\n------slack-secret-enc.yaml\n</code></pre> Content <pre><code>apiVersion: v1\ndata:\n  token: &lt;token&gt;\nkind: Secret\nmetadata:\n  name: slack-secret\n  namespace: flux-system\ntype: Opaque\n</code></pre> Make sure to encrypt the secret using sealed-secrets</p> <ol> <li>Create slack provider Directory structure <pre><code>clusters\n--production\n----flux-system\n------slack-secret-enc.yaml\n------notification-provider-slack.yaml\n</code></pre></li> </ol> <p>Content <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: notification-provider-slack\n  namespace: flux-system\nspec:\n  address: https://slack.com/api/chat.postMessage\n  channel: flux-notifications\n  secretRef:\n    name: slack-secret\n  type: slack\n  username: FluxCD\n</code></pre> 8. Create Alert resource to specify on which events we would like to get notified</p> <p>Directory structure <pre><code>clusters\n--production\n----flux-system\n------slack-secret-enc.yaml\n------notification-provider-slack.yaml\n------notification-alert-slack.yaml\n</code></pre> Content <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: notification-alert-slack\n  namespace: flux-system\nspec:\n  eventSeverity: info\n  eventSources:\n  - kind: Kustomization\n    name: '*'\n  - kind: GitRepository\n    name: '*'\n  - kind: HelmChart\n    name: '*'\n  - kind: HelmRepository\n    name: '*'\n  - kind: HelmRelease\n    name: '*'\n  - kind: ImageRepository\n    name: '*'\n  - kind: ImagePolicy\n    name: '*'\n  - kind: ImageUpdateAutomation\n    name: '*'\n  providerRef:\n    name: notification-provider-slack\n</code></pre> That would be it. Now we should be able to see Flux notifications in #flux-notifications slack channel</p> <p></p>"},{"location":"technical-guides/helmrepositories/","title":"Helm Repositores","text":"<p>In FluxCD, HelmRepositories is a custom resource definition (CRD) used to manage Helm repositories. Helm repositories are collections of Helm charts, which are packages of pre-configured Kubernetes resources. By defining a Helm repository using HelmRepositories, FluxCD can access and sync Helm charts from that repository into your Kubernetes cluster.</p>"},{"location":"technical-guides/helmrepositories/#bitnami-helm-repository","title":"Bitnami Helm Repository","text":"<p>Let's create a Helm Repository for Bitnami. They have comprehensive suite of helm charts like MySQL, Keycloak etc. </p> <p>Directory Structure of paas-config repository</p> <pre><code>clusters\n--production\n----flux-system\n------gotk-components.yaml\n------gotk-sync.yaml\n------kustomization.yaml\n------helmrepositories\n--------bitnami.yaml\n</code></pre> <p>Contenat of bitnami.yaml <pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: bitnami\n  namespace: flux-system\nspec:\n  interval: 1h0m0s\n  url: https://charts.bitnami.com/bitnami\n</code></pre> You can use the following command to generate the above manifest.  <pre><code>flux create source helm bitnami --url=https://charts.bitnami.com/bitnami --interval=10m --export\n</code></pre></p> <p>As you can see we have kustomization.yaml file inside flux-system directory. (which was created during the bootstrap). This file helps kuztomisation controller to build the content. We need to modify that too to include our new file. </p> <p>Content of kustomization.yaml <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - gotk-components.yaml\n  - gotk-sync.yaml\n  - helmrepositories/bitnami.yaml\n</code></pre></p> <p>Now commit these changes and push it to the main branch. FluxCD will automatically create the resources for you.  You can use the following commands to view the progress. </p> <p><pre><code>kubectl get helmrepositories -n flux-system\n</code></pre> We can also see kustomize controller logs to see the progress. </p> <p><pre><code>kubectl get pods -n flux-system\nNAME                                       READY   STATUS    RESTARTS   AGE\nhelm-controller-74b9b95b88-hc6rx           1/1     Running   0          7d18h\nkustomize-controller-696657b79c-fwlp9      1/1     Running   0          7d18h\nnotification-controller-6cb7b4f4bf-z4f7w   1/1     Running   0          7d18h\nsource-controller-5c69c74b57-6n47k         1/1     Running   0          7d18h\n</code></pre> Now we can see the logs.  <pre><code>kubectl logs -f kustomize-controller-696657b79c-fwlp9 -n flux-system\n</code></pre></p>"},{"location":"technical-guides/ingressgateway/","title":"Configuring Istio IngressGatway with Let's Encrypt Certificate","text":"<p>Now let's jump into the interesting bit. Let's create a sample nginx deployment and expose it through Istio Ingressgateway over TLS. </p>"},{"location":"technical-guides/ingressgateway/#dns-changes","title":"DNS changes","text":"<p>Determine the public IP address of istio-ingressgateway load balancer</p> <pre><code>kubectl get service istio-ingressgateway -n istio-system\n</code></pre> <pre><code>NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP    PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.0.41.165   4.250.87.120   15021:32693/TCP,80:32134/TCP,443:32131/TCP   8d\n</code></pre> <p>Point the domain to the external IP address. In our case, we have pointed both kubeflex.io and www.kubeflex.io to 4.250.87.120. </p>"},{"location":"technical-guides/ingressgateway/#create-a-test-deployment","title":"Create a test deployment","text":"<p>In our case, we have exposed our frontend service through the ingressgateway. But for simplicity, let's expose an nginx deployment through the ingressgateway. </p> <p>Directory structure <pre><code>clusters\n--production\n----frontend\n------deployment.yaml\n------service.yaml\n------namespace.yaml\n------service-account.yaml\n</code></pre> Content of namespace.yaml</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: frontend\n  labels:\n    istio-injection: enabled\n</code></pre> <p>Note that, we have enabled istio-injection in this namespace as the traffic in and out of this namespace matter to us.  Also it is important to create service accounts for each application as it provides an identity for the pods in the service mesh. </p> <p>Content of service-account.yaml <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: frontend\n  namespace: frontend\n</code></pre></p> <p>Content of deployment.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: frontend\n  namespace: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      serviceAccountName: frontend\n      - image: nginx\n        name: nginx\n        resources: {}\nstatus: {}\n</code></pre> <p>Content of service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: frontend\n  namespace: frontend\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  type: ClusterIP\n</code></pre></p> <p>Merge above changes to the main branch of the cluster-config repository. FluxCD will do the deployment. </p>"},{"location":"technical-guides/ingressgateway/#istio-configuration","title":"Istio Configuration","text":"<p>Now we need to create a gateway resource which describes how the load balancer at the edge of the mesh handles incoming traffic. </p> <p>Directory structure <pre><code>clusters\n--production\n----istio-system\n------gateway.yaml\n</code></pre></p> <p>Content of gateway.yaml <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - '*'\n</code></pre> For now, we are allowing HTTP traffic until we create the certificate using Let's Encrypt CRDs. </p> <p>In addition to that, we need a Virtual Service, which describes how the traffic is routed. </p> <p>Directory structure <pre><code>clusters\n--production\n----istio-system\n------gateway.yaml\n------virtual-service.yaml\n</code></pre> Content of virtual-service.yaml <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: frontend\n  namespace: istio-system\nspec:\n  hosts:\n    - \"www.kubeflex.io\"\n    - \"kubeflex.io\"\n  gateways:\n    - gateway\n  http:\n    - route:\n        - destination:\n            host: frontend.frontend.svc.cluster.local\n            port:\n              number: 80\n</code></pre> This routes the traffic receives by the gateway to the frontend service if the host maches with the above specification. </p> <p>Merge the above changes, so that FluxCD can do the deployment. Once the deployment is done, we are able to access the deployment via http://kubeflex.io endpoint. </p>"},{"location":"technical-guides/ingressgateway/#lets-encrypt-tls-certificate","title":"Let's Encrypt TLS Certificate","text":"<p>Now let's create a certificate resource. </p> <p>Directory structure <pre><code>clusters\n--production\n----istio-system\n------certificate.yaml\n</code></pre> Content of certificate.yaml <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: kubeflex\n  namespace: istio-system\nspec:\n  secretName: kubeflex-tls\n  duration: 2160h # 90d\n  renewBefore: 360h # 15d\n  isCA: false\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n  usages:\n    - server auth\n    - client auth\n  dnsNames:\n    - \"kubeflex.io\"\n    - \"www.kubeflex.io\"\n  issuerRef:\n    name: letsencrypt-prod-cluster\n    kind: ClusterIssuer\n    group: cert-manager.io\n</code></pre></p> <p>Once this is merged, cert-manager will create the certificate and store it in \"kubeflex-tls\" secret. </p> <p>We can see the progress with the following commands</p> <p><pre><code>kubectl get certificates -A\nkubectl get certificaterequets -A\n</code></pre> Now that we have the certificate, we can modify the gateway resource. </p> <p>Content of gateway.yaml <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - '*'\n    - port:\n        number: 443\n        name: https\n        protocol: HTTPS\n      tls:\n        mode: SIMPLE\n        credentialName: kubeflex-tls\n      hosts:\n      - \"www.kubeflex.io\"\n      - \"kubeflex.io\"\n</code></pre></p> <p>Once the above changes are merged, we are able to access the deployment via https://kubeflex.io or https://www.kubeflex.io</p>"},{"location":"technical-guides/istio/","title":"Istio","text":"<p>Istio is an open-source service mesh platform that provides a uniform way to connect, manage, and secure microservices. It allows you to connect, secure, control, and observe microservices, regardless of the underlying infrastructure. Istio extends the capabilities of Kubernetes to manage and orchestrate microservices by adding features like traffic management, security, policy enforcement, and telemetry.</p> <p>Now that we already have configured FluxCD, the installation is fairely easy. </p>"},{"location":"technical-guides/istio/#istio-system-namespace","title":"Istio-system Namespace","text":"<p>Create a manifest so that FluxCD can create istio-system namespace</p> <pre><code>clusters\n--production\n----istio-system\n------namespace.yaml\n</code></pre> <p>Content of namespace.yaml <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: istio-system\n</code></pre></p>"},{"location":"technical-guides/istio/#helm-repository","title":"Helm Repository","text":"<p>Create a HelmRepository manifest file inside istio-system directory</p> <pre><code>clusters\n--production\n----istio-system\n------namespace.yaml\n------helm-istio-repository.yaml\n</code></pre> <p>Content of helm-istio-repository.yaml <pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: istio\n  namespace: istio-system\nspec:\n  interval: 5m\n  url: https://istio-release.storage.googleapis.com/charts\n</code></pre> You can generate the above manifest with the following command</p> <pre><code>flux create source helm istio --url=https://istio-release.storage.googleapis.com/charts --interval=10m --namespace istio-system --export\n</code></pre>"},{"location":"technical-guides/istio/#install-istio-base-chart","title":"Install Istio Base Chart","text":"<p>To install Istio Base chart, we use HelmRelease CRD as follows</p> <pre><code>clusters\n--production\n----istio-system\n------namespace.yaml\n------helm-istio-repository.yaml\n------helm-release-istio-base.yaml\n</code></pre> <p>Content of helm-release-istio-base.yaml <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: istio-base\n  namespace: istio-system\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: base\n      sourceRef:\n        kind: HelmRepository\n        name: istio\n        namespace: istio-system\n      interval: 1m\n</code></pre> You can generate the above manifest with the following command</p> <pre><code>flux create helmrelease istio-base --namespace istio-system --source=HelmRepository/istio --chart=base --export\n</code></pre>"},{"location":"technical-guides/istio/#install-istiod","title":"Install istiod","text":"<p>Similarly, install istiod</p> <pre><code>clusters\n--production\n----istio-system\n------namespace.yaml\n------helm-istio-repository.yaml\n------helm-release-istio-base.yaml\n------helm-release-istiod.yaml\n</code></pre> <p>Content of helm-release-istiod.yaml <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: istiod\n  namespace: istio-system\nspec:\n  interval: 5m\n  dependsOn:\n    - name: istio-base\n      namespace: istio-system\n  chart:\n    spec:\n      chart: istiod\n      sourceRef:\n        kind: HelmRepository\n        name: istio\n      interval: 1m\n</code></pre> You can generate the above manifest with the following command</p> <pre><code>flux create helmrelease istiod --namespace istio-system --source=HelmRepository/istio --chart=istiod --export\n</code></pre>"},{"location":"technical-guides/istio/#install-istio-ingressgateway","title":"Install istio-ingressgateway","text":"<p>We will configuring the istio-ingressgateway so that we can ingest the external traffic</p> <pre><code>clusters\n--production\n----istio-system\n------namespace.yaml\n------helm-istio-repository.yaml\n------helm-release-istio-base.yaml\n------helm-release-istiod.yaml\n------helm-release-istio-gateway.yaml\n</code></pre> <p>Content of helm-release-istio-gateway.yaml</p> <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: istio-ingressgateway\n  namespace: istio-system\nspec:\n  interval: 5m\n  dependsOn:\n    - name: istio-base\n      namespace: istio-system\n    - name: istiod\n      namespace: istio-system\n  chart:\n    spec:\n      chart: gateway\n      sourceRef:\n        kind: HelmRepository\n        name: istio\n        namespace: istio-system\n      interval: 1m\n</code></pre> <p>Now we can commit these and push the changes to the main branch. FluxCD will do the thing. As of now, ingressgateway is still not able to accept the traffic. We will configure this in a later section. </p>"},{"location":"technical-guides/keycloak/","title":"Keycloak Deployment","text":"<p>Keycloak is an open-source identity and access management solution that enables secure authentication, authorization, and single sign-on for web applications and services. This document describes how we have deployed keycloak on kubernetes cluster and how we can expose it via Istio ingressgateway. Ideally this should sit inside the cluster with no public access.  It is possible that we may have overlooked some of the best practices during this deployment. We highly appreciate your feedback on this. </p>"},{"location":"technical-guides/keycloak/#create-namespaces-and-sas","title":"Create namespaces and SAs","text":"<p>As usual, Let's create two namespaces. One for the keycloak database, and the other one for the keycloak deployment. We thought of deploying the database in a separate namespace so that we can better visualize the traffic. </p> <p>Directory structure <pre><code>clusters\n--production\n----keycloak\n------namespace.yaml\n------service-account.yaml\n----keycloakdb\n------namespace.yaml\n------service-account.yaml\n</code></pre></p> <p>Content of keycloak/namespace.yaml <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: keycloak\n  labels:\n    istio-injection: enabled\n</code></pre> Content of keycloak/service-account.yaml <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: keycloak\n  namespace: keycloak\n</code></pre></p> <p>Content of keycloakdb/namespace.yaml <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: keycloakdb\n  labels:\n    istio-injection: enabled\n</code></pre> Content of keycloakdb/service-account.yaml <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: keycloakdb\n  namespace: keycloakdb\n</code></pre></p>"},{"location":"technical-guides/keycloak/#creating-secrets","title":"Creating Secrets","text":"<p>We need to create secrets for the database (MySQL) root password and database keycloak user password. In addition to that, we needt o create a secret for Keycloak admin user password. Keycloak database user password needs to be created in both the namespaces as secrets cannot be shared. </p> <p>Directory structure <pre><code>clusters\n--production\n----keycloak\n------namespace.yaml\n------service-account.yaml\n------secret-enc.yaml\n----keycloakdb\n------namespace.yaml\n------service-account.yaml\n------secret-enc.yaml\n</code></pre></p> <p>Content of keycloak/secret-enc.yaml <pre><code>---\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/cluster-wide: \"true\"\n  creationTimestamp: null\n  name: keycloak\n  namespace: keycloak\nspec:\n  encryptedData:\n    admin-password: &lt;enc&gt;\n    db-password: &lt;enc&gt;\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/cluster-wide: \"true\"\n      creationTimestamp: null\n      name: keycloak\n      namespace: keycloak\n</code></pre> Please note that this secret has been encrypted with sealed-secrets. Please see this for more information. </p> <p>admin-password would be the admin password of keycloak deployment. db-password is the keycloak database user password. </p> <p>Content of keycloakdb/secret-enc.yaml <pre><code>---\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/cluster-wide: \"true\"\n  creationTimestamp: null\n  name: keycloakdb\n  namespace: keycloakdb\nspec:\n  encryptedData:\n    mysql-password: &lt;enc&gt;\n    mysql-root-password: &lt;enc&gt;\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/cluster-wide: \"true\"\n      creationTimestamp: null\n      name: keycloakdb\n      namespace: keycloakdb\n</code></pre></p> <p>Where mysql-password is the keycloak mysql user password. mysql-root-passowrd is the root password of the MySQL deployment. </p>"},{"location":"technical-guides/keycloak/#mysql-database-deployment","title":"MySQL Database Deployment","text":"<p>We deploy MySQL using a helmrelease</p> <p>Directory structure</p> <p><pre><code>clusters\n--production\n----keycloak\n------namespace.yaml\n------service-account.yaml\n------secret-enc.yaml\n----keycloakdb\n------namespace.yaml\n------service-account.yaml\n------secret-enc.yaml\n------database.yaml\n</code></pre> Content of database.yaml <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: keycloakdb\n  namespace: keycloakdb\nspec:\n  chart:\n    spec:\n      chart: mysql\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami\n        namespace: flux-system\n      version: 10.1.0\n  interval: 1m0s\n  timeout: 20m\n  targetNamespace: keycloakdb\n  values:\n    auth:\n      createDatabase: true\n      database: keycloakdb\n      username: keycloak-user\n      existingSecret: keycloakdb\n    serviceAccount:\n      name: keycloakdb\n      create: false\n    image:\n      debug: true\n</code></pre> Note: Ideally, we should mention the resources (limits and requests) in the manifest. </p>"},{"location":"technical-guides/keycloak/#keycloak-deployment_1","title":"Keycloak Deployment","text":"<p>We were not able to find a helm chat which supports MySQL. Currently available charts support only PostgreSQL as the backend. Therefore we go ahead with plain Kubernetes manifest to deploy Keycloak. </p> <pre><code>clusters\n--production\n----keycloak\n------namespace.yaml\n------service-account.yaml\n------secret-enc.yaml\n------keycloak.yaml\n------service.yaml\n----keycloakdb\n------namespace.yaml\n------service-account.yaml\n------secret-enc.yaml\n------database.yaml\n</code></pre> <p>Content of keycloak.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keycloak\n  namespace: keycloak\n  labels:\n    app: keycloak\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keycloak\n  template:\n    metadata:\n      labels:\n        app: keycloak\n    spec:\n      serviceAccountName: keycloak\n      automountServiceAccountToken: true\n      containers:\n        - name: keycloak\n          image: quay.io/keycloak/keycloak:20.0.2\n          args: [\"start\"]\n          env:\n            - name: KEYCLOAK_ADMIN\n              value: \"admin\"\n            - name: KEYCLOAK_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: keycloak\n                  key: admin-password\n            - name: KC_HOSTNAME\n              value: keycloak.kubeflex.io\n            - name: KC_PROXY\n              value: \"edge\"\n            - name: KC_DB\n              value: mysql\n            - name: KC_DB_URL\n              value: \"jdbc:mysql://keycloakdb-keycloakdb-mysql.keycloakdb.svc.cluster.local:3306/keycloakdb\"\n            - name: KC_DB_USERNAME\n              value: \"keycloakapp-user\"\n            - name: jgroups.dns.query\n              value: keycloak\n            - name: KC_DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: keycloak\n                  key: db-password\n          ports:\n            - name: http\n              containerPort: 8080\n            - name: jgroups\n              containerPort: 7600\n</code></pre></p> <p>Content of service.yaml <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: keycloak\n  namespace: keycloak\n  labels:\n    app: keycloak\nspec:\n  ports:\n    - name: http\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: keycloak\n  type: ClusterIP\n</code></pre></p> <p>FluxCD can deploy the reources once the changes are merged into the main branch. </p>"},{"location":"technical-guides/keycloak/#keycloak-ingress","title":"Keycloak ingress","text":"<p>Now let's create a DNS entry to expose keycloak via ingressgateway. </p> <p>Determine the ingressgateway public IP address. </p> <pre><code>kubectl get service istio-ingressgateway -n istio-system\nistio-ingressgateway   LoadBalancer   10.0.41.165   4.250.87.120   15021:32693/TCP,80:32134/TCP,443:32131/TCP   8d\n</code></pre> <p>Create a DNS entry</p> <p>keycloak.kubeflex.io -&gt; 4.250.87.120</p>"},{"location":"technical-guides/keycloak/#gateway-changes","title":"Gateway changes","text":"<p>Gateway still serves http on port 80. </p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - '*'\n</code></pre> <p>Create a virtual service to route the traffic to the keycloak service</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: keycloak\n  namespace: istio-system\nspec:\n  hosts:\n    - \"keycloak.kubeflex.io\"\n  gateways:\n    - gateway\n  http:\n    - route:\n        - destination:\n            host: keycloak.keycloak.svc.cluster.local\n            port:\n              number: 8080\n</code></pre> <p>Now we are ready to create the TLS certificate</p>"},{"location":"technical-guides/keycloak/#certificate","title":"Certificate","text":"<p>Create a certificate resource. </p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: kubeflex\n  namespace: istio-system\nspec:\n  secretName: keycloak-kubeflex-tls\n  duration: 2160h # 90d\n  renewBefore: 360h # 15d\n  isCA: false\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n  usages:\n    - server auth\n    - client auth\n  dnsNames:\n    - \"keycloak.kubeflex.io\"\n  issuerRef:\n    name: letsencrypt-prod-cluster\n    kind: ClusterIssuer\n    group: cert-manager.io\n</code></pre> <p>Once the above changes are merged, the certificate information will be stored into keycloak-kubeflex-tls secret. </p> <p>Let's modify the gateway resource to use the certificate. </p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - '*'\n    - port:\n        number: 443\n        name: https\n        protocol: HTTPS\n      tls:\n        mode: SIMPLE\n        credentialName: keycloak-kubeflex-tls\n      hosts:\n      - \"keycloak.kubeflex.io\"\n</code></pre> <p>With the above change, we can access our keycloak via https://keycloak.kubeflex.io. </p>"},{"location":"technical-guides/local/","title":"Local Development Setup","text":"<p>We initially started using minikube as the local development environment but we later realised that it adds an extra layer of complexity to our developers. Therefore we resorted to a setup powered by docker-compose. </p> <p>Sample directory structure <pre><code>kubeflex-platform\n--auth-service\n----Dockerfile\n--frontend\n----Dockerfile\n--backend\n----Dockerfile\n--docker-compose.yaml\n</code></pre></p> <p>Sample content of docker-compose.yaml file <pre><code>services:\n  frontend:\n    build: ./frontend\n    ports:\n      - \"3000:3000\"\n    environment:\n      BACKEND_SERVICE: backend\n      BACKEND_SERVICE_PORT: 8080\n  backend:\n    build: ./backend\n    ports:\n      - \"8080:8080\"\n    environment:\n      SERVICE_PORT: 8080\n      MONGODB_URI: mongodb+srv://&lt;user&gt;:&lt;password&gt;@&lt;cluster-name&gt;.xxx.mongodb.net/\n  auth-service:\n    build: ./auth-service\n    ports:\n      - \"3200:3200\"\n    environment:\n      SERVICE_PORT: 3200\n      MYSQL_HOST: mysqldb\n      MYSQL_PORT: 3306\n      MYSQL_USER: dev-user\n      MYSQL_PASSWD: devpassword\n  mysqldb:\n    image: mysql:8.0.36\n    ports:\n      - 3306:3306\n    restart: unless-stopped\n    environment:\n      MYSQL_USER: keycloakapp-user\n      MYSQL_PASSWORD: keycloak123\n      MYSQL_DATABASE: keycloakdb\n      MYSQL_ROOT_PASSWORD: devpassword\n    volumes:\n      - ./mysql_data:/var/lib/mysql\n  keycloak:\n    image: quay.io/keycloak/keycloak:24.0\n    command: start-dev\n    ports:\n      - 9081:8080\n    restart: unless-stopped\n    environment:\n      KEYCLOAK_ADMIN: admin\n      KEYCLOAK_ADMIN_PASSWORD: admin@1234\n      KC_DB: mysql\n      KC_DB_USERNAME: keycloakapp-user\n      KC_DB_PASSWORD: keycloak123\n      KC_DB_URL: jdbc:mysql://mysqldb:3306/keycloakdb\n      KC_HOSTNAME: localhost\n    depends_on:\n      - mysqldb\nvolumes:\n  mysql_data:\n</code></pre></p> <p>These exact environment variable keys are available in kubernetes in form of secrets (with different values of course)</p>"},{"location":"technical-guides/minio/","title":"MinIO Deployment","text":"<p>MinIO is a high-performance, S3 compatible object store. This gives us the flexibility to move our object storage to any cloud provider in the future. </p>"},{"location":"technical-guides/minio/#helm-repository","title":"Helm Repository","text":"<p>Directory structure <pre><code>clusters\n--production\n----flux-system\n------helm-repositories\n--------bitnami.yaml\n------kustomization.yaml\n</code></pre> Content of bitnami.yaml <pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: bitnami\n  namespace: flux-system\nspec:\n  interval: 1h0m0s\n  url: https://charts.bitnami.com/bitnami\n</code></pre></p> <p>Content of kustomisation.yaml <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - gotk-components.yaml\n  - gotk-sync.yaml\n  - helmrepositories/sealed-secrets.yaml\n  - helmrepositories/cert-manager.yaml\n  - helmrepositories/bitnami.yaml\n</code></pre></p>"},{"location":"technical-guides/minio/#deploying-minio","title":"Deploying MinIO","text":"<p>Directory structure <pre><code>clusters\n--production\n----minio\n------minio.yaml\n------namespace.yaml\n------secret-enc.yaml\n------service-account.yaml\n</code></pre></p> <p>Content of minio.yaml <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  chart:\n    spec:\n      chart: minio\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami\n        namespace: flux-system\n      version: 14.1.4\n  interval: 1m0s\n  timeout: 20m\n  targetNamespace: minio\n  values:\n    auth:\n      existingSecret: minio\n    defaultBuckets: \"kubeflex\"\n    resources:\n      limits:\n      requests:\n        cpu: 10m\n        memory: 40Mi\n    serviceAccount: \n      create: false\n      name: \"minio\"\n</code></pre> Content of namespace.yaml <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: minio\n  labels:\n    istio-injection: enabled\n</code></pre> Content of service-account.yaml <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: minio\n  namespace: minio\n</code></pre></p> <p>Content of secret-enc.yaml <pre><code>---\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/cluster-wide: \"true\"\n  creationTimestamp: null\n  name: minio\n  namespace: minio\nspec:\n  encryptedData:\n    root-password: &lt;enc&gt;\n    root-user: &lt;enc&gt;\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/cluster-wide: \"true\"\n      creationTimestamp: null\n      name: minio\n      namespace: minio\n</code></pre></p> <p>Please note that the secret has been encrypted using sealed-secrets. Please see here.  It is important to note that, we need to have root-password and root-user as keys in the secret. </p> <p>Once the above files are pushed to the main branch, FluxCD will deploy MinIO. Please note that if we want high availability feature, the deployment will be a little different. </p>"},{"location":"technical-guides/monitoring/","title":"Monitoring Tools","text":"<p>Monitoring Tools Installation: While this could have been done with a more systematic approach using HelmReleases, we have chosen to install monitoring tools such as Prometheus, Grafana, Kiali, and Jaeger using plain Kubernetes manifests that come with the Istio installation package for the sake of expediency.</p>"},{"location":"technical-guides/monitoring/#installation","title":"Installation","text":"<p>Directory structure <pre><code>clusters\n--production\n----istio-system\n------kiali.yaml\n------prometheus.yaml\n------grafana.yaml\n------jaeger.yaml\n</code></pre></p> <p>Download the manifests:</p> <p><pre><code>https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/grafana.yaml\nhttps://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/jaeger.yaml\nhttps://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/kiali.yaml\nhttps://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/prometheus.yaml\n</code></pre> Currently we access these ClusterIP services with port-forwarding.  We intend to convert these deployments into helmreleases soon. </p>"},{"location":"technical-guides/release/","title":"Conventional Commits, Semantic Versioning and CircleCI","text":"<p>This document describes how we have implemented the release process. This is a combination of following standards/conventions/technologies. 1. Conventional Commits 2. GitHub Actions 3. Semantic Versioning 4. CircleCI 5. FluxCD</p>"},{"location":"technical-guides/release/#conventional-commits","title":"Conventional Commits","text":"<p>Conventional Commits is a specification for structuring commit messages in software development. It defines a set of rules for creating commit messages that convey meaning about the changes made in the commit. This standardization helps in automating versioning, generating changelogs, and facilitating collaboration among developers. See here</p>"},{"location":"technical-guides/release/#semantic-versioning","title":"Semantic Versioning","text":"<p>Semantic versioning (often abbreviated as SemVer) is a versioning scheme used in software development to convey meaning about the changes in a software application or library. It consists of three numbers separated by periods: MAJOR.MINOR.PATCH. See here</p>"},{"location":"technical-guides/release/#the-repository","title":"The repository","text":"<p>Let's use our bookinfo node repository as the example. We already have package.json file at the root of the repository so that we can track our release version. If this is not a node repository, we can alternatively add a package.json file as follows <pre><code>{\n  \"name\": \"bookinfo\",\n  \"version\": \"1.0.0\"\n}\n</code></pre></p> <p>In order to automate the release creation, we use release-please GitHub action. Add the following content to .github/workflows/release.yml file. </p> <p><pre><code>name: Release\n\non:\n  push:\n    branches:\n      - main\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '21'\n\n      - name: Create Release\n        uses: google-github-actions/release-please-action@v4\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          release-type: node\n</code></pre> So, this GitHub action does the following things.  1. Versioning: It determines the next version number based on the changes introduced in the pull requests since the last release. It follows Semantic Versioning rules to increment the version number accordingly (major, minor, or patch). 2. Release Notes: It generates release notes by compiling the descriptions of pull requests merged since the last release. These release notes provide a summary of the changes included in the release, helping users understand what has been added, changed, or fixed. 3. GitHub Release: It creates a new release on GitHub with the calculated version number and the generated release notes. This automates the process of creating releases and ensures consistency across releases.</p> <p>Commit the above change with the following commit message, so that it will be picked up as a minor release.  <pre><code>feat: release process\n</code></pre></p> <p>Once the above change is pushed into the main branch, release please will create a new PR to bump the version.  </p> <p>Once the PR is merged, it will create a new release with the version v1.0.0.</p> <p></p> <p>Now, from this point onwards, release-please will honor our commit messages and it will create pull requests to bump the release version accordingly. </p>"},{"location":"technical-guides/release/#image-creation-with-circleci","title":"Image creation with CircleCI","text":"<p>Now we would like CircleCI to create images whenever a GitHub release is created. And also we would like to tag the image with the release version and push that into the image repository.  Add the following content to .circleci/config.yml file <pre><code>version: 2.1\n\njobs:\n  build-and-push:\n    docker:\n      - image: cimg/node:21.7.1\n    steps:\n      - setup_remote_docker\n      - checkout\n      - run:\n          name: Login to Azure Container Registry\n          command: docker login -u $AZURE_ACR_USERNAME -p $AZURE_ACR_PASSWORD $AZURE_ACR_LOGIN_SERVER\n      - run:\n          name: Build Docker image\n          command: |\n            docker build -t $CIRCLE_PROJECT_REPONAME .\n      - run:\n          name: Tag Docker image\n          command: |\n            docker tag $CIRCLE_PROJECT_REPONAME $AZURE_ACR_LOGIN_SERVER/$CIRCLE_PROJECT_REPONAME:$CIRCLE_TAG\n      - run:\n          name: Push Docker image to Azure Container Registry\n          command: |\n            docker push $AZURE_ACR_LOGIN_SERVER/$CIRCLE_PROJECT_REPONAME:$CIRCLE_TAG\n\nworkflows:\n  version: 2\n  build-and-push-image:\n    jobs:\n      - build-and-push:\n          context:\n            - registry\n          filters:\n            branches:\n              ignore: /.*/\n            tags:\n              only: /^v.*/\n</code></pre> So, this instructs CircleCI to build and push the image to our container registry, whenever a release is created. Please see CircleCI docs to learn more about CircleCI built-in variable CIRCLE_TAG.</p> <p></p>"},{"location":"technical-guides/release/#fluxcd-image-reflector-and-automation-controllers","title":"FluxCD image reflector and automation controllers","text":"<p>Now, at this point we already have the image in our image repository. At the same time, we already have configured FluxCD so that it reconcile the changes whenever we modify the manifests in paas-cluster repository.  Below is the respective manifest for our bookinfo application.  Content of deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bookinfo\n  namespace: bookinfo\n  labels:\n    app: bookinfo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bookinfo\n  template:\n    metadata:\n      labels:\n        app: bookinfo\n        sidecar.istio.io/inject: \"true\"\n    spec:\n      serviceAccountName: bookinfo\n      automountServiceAccountToken: true\n      containers:\n        - name: bookinfo\n          image: acr.kubeflex.io/bookinfo:v1.0.0 \n          env:\n            - name: DB_HOST\n              value: \"bookinfo-db.bookinfo.svc.cluster.local\"\n</code></pre> However, we need to manually modify the version of the image to get FluxCD to pick the changes. We can automate this by using FluxCD image reflector and automation controllers.  Inspect the controllers we have installed at this point. See here for the initial bootstrap steps.  <pre><code>kubectl get deployments -n flux-system\nNAME                          READY   UP-TO-DATE   AVAILABLE   AGE\nhelm-controller               1/1     1            1           43d\nkustomize-controller          1/1     1            1           43d\nnotification-controller       1/1     1            1           43d\nsource-controller             1/1     1            1           43d\n</code></pre> Now we need to install image automation controller and image reflector controller. We can use the same bootstrap command with an additional argument as follows <pre><code>flux bootstrap github \\\n  --token-auth \\\n  --owner=kubeflex \\\n  --repository=paas-config \\\n  --branch=main \\\n  --path=clusters/production \\\n  --personal\n  --components-extra=\"image-reflector-controller,image-automation-controller\"\n</code></pre> Verify that the controllers have been installed.  <pre><code>kubectl get deployments -n flux-system\nNAME                          READY   UP-TO-DATE   AVAILABLE   AGE\nhelm-controller               1/1     1            1           43d\nimage-automation-controller   1/1     1            1           38h\nimage-reflector-controller    1/1     1            1           38h\nkustomize-controller          1/1     1            1           43d\nnotification-controller       1/1     1            1           43d\nsource-controller             1/1     1            1           43d\n</code></pre> Create an image repository manifest to scan and store a specific set of tags in a database.  Directory structure <pre><code>clusters\n--production\n----flux-system\n------imagerepositories\n--------bookinfo.yaml\n</code></pre> Content <pre><code>apiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageRepository\nmetadata:\n  name: bookinfo\n  namespace: flux-system\nspec:\n  secretRef:\n    name: acr-secret\n  image: acr.kubeflex.io/bookinfo\n  interval: 5m\n</code></pre></p> <p>Now let's create an imagepolicy resource which defines rules for selecting a latest image from ImageRepositories.  Directory structure <pre><code>clusters\n--production\n----flux-system\n------imagerepositories\n--------bookinfo.yaml\n------imagepolicies\n--------bookinfo.yaml\n</code></pre> Content <pre><code>apiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImagePolicy\nmetadata:\n  name: bookinfo\n  namespace: flux-system\nspec:\n  imageRepositoryRef:\n    name: bookinfo\n  policy:\n    semver:\n      range: '&gt;=1.0.0'\n</code></pre> Next, let's define ImageUpdateAutomation resource which defines an automation process that will update a git repository, based on image policy objects in the same namespace.  Directory structure <pre><code>clusters\n--production\n----flux-system\n------imagerepositories\n--------bookinfo.yaml\n------imagepolicies\n--------bookinfo.yaml\n----image-update-automation.yaml\n</code></pre> Content <pre><code>apiVersion: image.toolkit.fluxcd.io/v1beta1\nkind: ImageUpdateAutomation\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 30m\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  git:\n    checkout:\n      ref:\n        branch: main\n    commit:\n      author:\n        email: fluxcdbot@users.noreply.github.com\n        name: fluxcdbot\n      messageTemplate: '{{range .Updated.Images}}{{println .}}{{end}}'\n    push:\n      branch: main\n  update:\n    path: ./clusters/production\n    strategy: Setters\n</code></pre> See the docs</p> <p>Finally we need to specify which specific location FluxCD should modify whenever a new image is available in remote image repository. We can do this by adding a comment to the deployment manifest.  <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bookinfo\n  namespace: bookinfo\n  labels:\n    app: bookinfo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bookinfo\n  template:\n    metadata:\n      labels:\n        app: bookinfo\n        sidecar.istio.io/inject: \"true\"\n    spec:\n      serviceAccountName: bookinfo\n      automountServiceAccountToken: true\n      containers:\n        - name: bookinfo\n          image: eocontainerregistry.azurecr.io/bookinfo:v1.1.1 # {\"$imagepolicy\": \"flux-system:bookinfo\"}\n          env:\n            - name: DB_HOST\n</code></pre></p> <p>Now, whenever a new image is pushed to the image repository, fluxcd will modify the image tag in the deployment manifest. This change will again reconciled by FluxCD and a new deployment will happen as part of Flux reconciliation process. </p>"},{"location":"technical-guides/requestauthentication/","title":"Request Authentication with Istio and Keycloak","text":"<p>RequestAuthentication defines what request request authentication methods are supported by a workload. Please see this</p>"},{"location":"technical-guides/requestauthentication/#scenairio","title":"Scenairio","text":"<p>Let's assume we have a service called skills-service which as two basic endpoints. This is deployed in skills-service namespace.  <pre><code>/addskill - This adds an skill to the database\n/getskills - This retrieves all the skills in the database.\n</code></pre></p> <p>We have following configuration in our keycloak deployment deployment. </p> <p>Realm name: istio</p> <p>Roles: admin, user</p> <p>Users: skill-admin, skill-user</p> <p>Role assignments:</p> <p>The role \"user\" has been assigned to \"skill-user\" user. </p> <p>THe role \"admin\" has been assigned to \"skill-admin\" user. </p>"},{"location":"technical-guides/requestauthentication/#requestauthorization","title":"RequestAuthorization","text":"<p>Directory structure <pre><code>clusters\n--production\n----skills-service\n-------deployment.yaml\n-------service.yaml\n-------service-account.yaml\n-------namespace.yaml\n-------request-authentication.yaml\n</code></pre></p> <p>Request Authentication manifest <pre><code>apiVersion: security.istio.io/v1beta1\nkind: RequestAuthentication\nmetadata:\n  name: skills-service-request-authentication\n  namespace: skills-service\nspec:\n  selector:\n     matchLabels:\n      app: skills-service\n  jwtRules:\n   - issuer: \"https://keycloak.kubeflex.io/realms/istio\"\n     jwksUri: \"https://keycloak.kubeflex.io/realms/istio/protocol/openid-connect/certs\"\n     outputPayloadToHeader: jwt-parsed\n</code></pre></p> <p>This will reject a request if the request contains invalid authentication information. However, a request which does not have any authentication information at all, will be accepted. To prevent this, we need to have an Authorization Policy.</p> <p>Directory structure <pre><code>clusters\n--production\n----skills-service\n-------deployment.yaml\n-------service.yaml\n-------service-account.yaml\n-------namespace.yaml\n-------request-authentication.yaml\n-------authorization-policy.yaml\n</code></pre> authorization-policy.yaml <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: skills-service-authorization-policy\n  namespace: skills-service\nspec:\n  selector:\n    matchLabels:\n      app: skills-service\n  rules:\n  - from:\n    - source:\n        requestPrincipals: [\"*\"]\n</code></pre></p> <p>Above configuration makes the authentication mandatory to call both /getskills and /addskill endpoints. However, with the current configuration, both \"skill-user\" and \"skill-admin\" can add skills. Let's change the authorization policy such a way that, only admin user can add skills. </p> <p>authorization-policy.yaml <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: skills-service-authorization-policy\n  namespace: skills-service\nspec:\n  selector:\n    matchLabels:\n       app: skills-service\n  rules:\n  - from:\n    - source:\n        requestPrincipals: [\"*\"]\n    to:\n    - operation:\n        methods: [\"GET\"]\n        paths: [\"/getskills\"]\n\n  - from:\n    - source:\n        requestPrincipals: [\"*\"]\n    to:\n    - operation:\n        methods: [\"POST\"]\n        paths: [\"/addskill*\"]\n    when:\n    - key: request.auth.claims[realm_access][roles]\n      values: [\"admin\"]\n</code></pre></p>"},{"location":"technical-guides/sealed-secrets/","title":"Sealed Secrets","text":"<p>Sealed Secrets is a Kubernetes tool that enhances security by enabling the encryption of Kubernetes Secrets at rest in a Git repository. It allows users to commit encrypted versions of their Secrets to source control while still maintaining the ability to decrypt and use them within the Kubernetes cluster. Sealed Secrets employs public-key cryptography, where the cluster holds the public key, enabling it to encrypt Secrets, while a client tool called \"kubeseal\" uses a private key to decrypt the Secrets for use within the cluster. This approach ensures that sensitive information, such as passwords or API keys, remains secure even when stored in version control.</p>"},{"location":"technical-guides/sealed-secrets/#helm-repository","title":"Helm Repository","text":"<p>Let's create a HelmRepository for sealed-secrets Directory structure <pre><code>clusters\n--production\n----flux-system\n----kustomization.yaml\n------helmrepositories\n--------sealed-secrets.yaml\n</code></pre></p> <p>The content of sealed-secrets.yaml <pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: sealed-secrets\n  namespace: flux-system\nspec:\n  interval: 1h0m0s\n  url: https://bitnami-labs.github.io/sealed-secrets\n</code></pre> The content of kustomization.yaml <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - gotk-components.yaml\n  - gotk-sync.yaml\n  - helmrepositories/sealed-secrets.yaml\n</code></pre></p>"},{"location":"technical-guides/sealed-secrets/#creating-the-namespace","title":"Creating the namespace","text":"<p>Lets create a namespace for sealed-secrets</p> <p>Directory structure <pre><code>clusters\n---production\n-----sealed-secrets\n-------namespace.yaml\n</code></pre> Content of namespace.yaml <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: sealed-secrets\n</code></pre> Note, we do not want to inject istio sidecar for this namespace. </p>"},{"location":"technical-guides/sealed-secrets/#creating-the-helmrelease","title":"Creating the helmrelease","text":"<p>Let's deploy sealed-secrets using a helmrelease CRD Directory structure <pre><code>clusters\n--production\n----sealed-secrets\n------namespace.yaml\n------sealed-secrets.yaml\n</code></pre></p> <p>Content of sealed-secrets.yaml <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: sealed-secrets\n  namespace: flux-system\nspec:\n  chart:\n    spec:\n      chart: sealed-secrets\n      sourceRef:\n        kind: HelmRepository\n        name: sealed-secrets\n      version: \"&gt;=1.15.0-0\"\n  interval: 1h0m0s\n  releaseName: sealed-secrets-controller\n  targetNamespace: sealed-secrets\n  install:\n    crds: Create\n  upgrade:\n    crds: CreateReplace\n</code></pre></p> <p>Push the above changes to the main branch of cluster-config repository and FluxCD will automatically do the sealed-secrets deployment</p>"},{"location":"technical-guides/sealed-secrets/#validation","title":"Validation","text":"<p>Let's validate what we have done by creating an encrypted secret. </p>"},{"location":"technical-guides/sealed-secrets/#create-a-namespace","title":"Create a namespace.","text":"<p>Folder structure <pre><code>clusters\n--production\n----test-namespace\n------namespace.yaml\n</code></pre> Content of namespace.yaml</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: test-namespace\n</code></pre>"},{"location":"technical-guides/sealed-secrets/#create-a-secret","title":"Create a secret","text":"<p>Create a secret in the test-namespace as follows <pre><code>clusters\n--production\n----test-namespace\n------namespace.yaml\n------secret.yaml\n</code></pre></p> <p>Generate the content of secret.yaml as follows <pre><code>kubectl create secret generic test-secret -n test-namespace --from-literal=password=password123 --dry-run=client -o yaml &gt; secret.yaml\n</code></pre></p> <p>The content of secret.yaml <pre><code>apiVersion: v1\ndata:\n  password: cGFzc3dvcmQxMjM=\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: test-secret\n  namespace: test-namespace\n</code></pre></p> <p>As you can see, the password value is base64 encoded, not encrypted. If we commit this file into the repository, anyone who has the access to the repository would be able to decode the secret as follows <pre><code>echo -n \"cGFzc3dvcmQxMjM=\" |base64 -d\n</code></pre></p>"},{"location":"technical-guides/sealed-secrets/#retrieving-the-public-certificate","title":"Retrieving the public certificate","text":"<p>In order to encrypt the secrets, we need to install kubeseal utility. You can see the latest releases here</p> <p>Now, retrieve the public certificate with kubeseal utility. Please note that the controller name depends on the helmrelease name.  <pre><code>kubeseal --controller-name=sealed-secrets-controller --controller-namespace=sealed-secrets --fetch-cert &gt; pub.crt\n</code></pre> Now that we have the public key, we can encrypt our secret.yaml</p> <p><pre><code>kubeseal -o yaml --scope cluster-wide --cert pub.crt &lt; secret.yaml &gt; secret-enc.yaml\n</code></pre> This will write the encrypted secret into secret-enc.yaml. </p> <p>Let's inspect the content</p> <pre><code>---\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/cluster-wide: \"true\"\n  creationTimestamp: null\n  name: test-secret\n  namespace: test-namespace\nspec:\n  encryptedData:\n    password: AgCrjD2fIvmCiOecKrCgdJxXoev2F4P5AbR+weqb7CdrCo+b0SfBHA/aCDnVbjF4GoJqcQk3xGUlHfNsbw7i0zVB5RxhCNziHwRyGI22rghjt6Wj7T04ko3U+YMJalYpM8MFk2Uy34abE08ygFzE491/XAy4Yym1hQEQAANu8iXH59r+3xOG+UhqycXYatJlstQnq51/7JQZ3+DbC54C6z6JereLv8EZmIXjiJ682jY/cA3clMoYWdksng/rAVQlJwgG7oRtJUFWAGhTiBMO4PBh5CDdQrcSpMUbEOYJDxewcFUswDZlWWA8h895wjnGdy0oRWc/+kOCt1Tm2KKKIPH2/y/HBcFTu5yw+nStQ+HKam0lZNF4i+uC18FSLP0RJp/VEsWXtBZlttluRCJQgy3l7wdc3EOtw0S5XAIq4KZaSFyTKRL56iiXZYxIrsBqhDCmxDxnUxoM2E7RjWBPziOPCogoWO+FsEX7HL0AC2+kj3K7nBJ6vWkfxXoitrItIfnDfkEoU9PlQVeqH8rv0JaKYcNqFlMQJBQdSJUsolEP7lU1IMGUcwDZwnSUwAMIH+fRqv+HD4AqgWMkb2v/PWKy1Oi728LterGrs9xtp1X9QuKF/yeZT7lxhdbNp3kIq9CLqDUGG/ifWKb/ovEUpXEItIbXKg+VYJHQ9lk6/gr7kOf5qWYz6xfy4jAae5ZjSFuel4k3qsbDA==\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/cluster-wide: \"true\"\n      creationTimestamp: null\n      name: test-secret\n      namespace: test-namespace\n</code></pre> <p>As you can see, the password now has been encrypted. </p> <p>Now we can safely commit secret-enc.yaml file to the repository. Kubernetes would be able to decrypt this secret as it already has sealed-secrets private key. </p>"},{"location":"technical-guides/sealed-secrets/#decrpting-the-secret","title":"Decrpting the secret","text":"<p>Once the changes are merged, FluxCD will create the secret in the kubernetes cluster. Now we can inspect the secret using kubectl utility</p> <pre><code>kubectl get secret test-secret -n test-namespace -o yaml\n</code></pre> <p>Output will be similar to the following. </p> <p><pre><code>apiVersion: v1\ndata:\n  password: cGFzc3dvcmQxMjM=\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: test-secret\n  namespace: test-namespace\n</code></pre> We can observe that Kubernetes was able to decrypt the secret. </p>"}]}